{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload \n",
    "import datacleaner\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "data =  pd.read_csv(\"C:\\\\Users\\\\user\\\\Desktop\\\\titanic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "data_up = data.drop([\"Name\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'datacleaner' from 'c:\\\\users\\\\user\\\\appdata\\\\local\\\\programs\\\\python\\\\python37\\\\lib\\\\site-packages\\\\datacleaner\\\\__init__.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload \n",
    "from importlib import reload \n",
    "reload(datacleaner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "datacleaner.autoclean(data_up,encoder=OneHotEncoder,encoder_kwargs={\"categories\":[\"Pclass\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd =datacleaner.autoclean(data_up,encoder=OneHotEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd[\"Sex\"].iloc[0].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Siblings/Spouses Aboard</th>\n",
       "      <th>Parents/Children Aboard</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Mr. Owen Harris Braund</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Mrs. John Bradley (Florence Briggs Thayer) Cum...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Miss. Laina Heikkinen</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Mrs. Jacques Heath (Lily May Peel) Futrelle</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Mr. William Henry Allen</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Mr. James Moran</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.4583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Mr. Timothy J McCarthy</td>\n",
       "      <td>male</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51.8625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Master. Gosta Leonard Palsson</td>\n",
       "      <td>male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Mrs. Oscar W (Elisabeth Vilhelmina Berg) Johnson</td>\n",
       "      <td>female</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11.1333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Mrs. Nicholas (Adele Achem) Nasser</td>\n",
       "      <td>female</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass                                               Name  \\\n",
       "0         0       3                             Mr. Owen Harris Braund   \n",
       "1         1       1  Mrs. John Bradley (Florence Briggs Thayer) Cum...   \n",
       "2         1       3                              Miss. Laina Heikkinen   \n",
       "3         1       1        Mrs. Jacques Heath (Lily May Peel) Futrelle   \n",
       "4         0       3                            Mr. William Henry Allen   \n",
       "5         0       3                                    Mr. James Moran   \n",
       "6         0       1                             Mr. Timothy J McCarthy   \n",
       "7         0       3                      Master. Gosta Leonard Palsson   \n",
       "8         1       3   Mrs. Oscar W (Elisabeth Vilhelmina Berg) Johnson   \n",
       "9         1       2                 Mrs. Nicholas (Adele Achem) Nasser   \n",
       "\n",
       "      Sex   Age  Siblings/Spouses Aboard  Parents/Children Aboard     Fare  \n",
       "0    male  22.0                        1                        0   7.2500  \n",
       "1  female  38.0                        1                        0  71.2833  \n",
       "2  female  26.0                        0                        0   7.9250  \n",
       "3  female  35.0                        1                        0  53.1000  \n",
       "4    male  35.0                        0                        0   8.0500  \n",
       "5    male  27.0                        0                        0   8.4583  \n",
       "6    male  54.0                        0                        0  51.8625  \n",
       "7    male   2.0                        3                        1  21.0750  \n",
       "8  female  27.0                        0                        2  11.1333  \n",
       "9  female  14.0                        1                        0  30.0708  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Survived                     int64\n",
       "Pclass                       int64\n",
       "Name                         int32\n",
       "Sex                          int32\n",
       "Age                        float64\n",
       "Siblings/Spouses Aboard      int64\n",
       "Parents/Children Aboard      int64\n",
       "Fare                       float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-1.0.2-py3-none-win_amd64.whl (24.6 MB)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from xgboost) (1.18.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from xgboost) (1.4.1)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#sys.path =\"C:\\\\Users\\\\user\\\\Desktop\\\\my-vishnu-proj'\\\\preprocessing\"\n",
    "#sys.path = \"C:\\\\Users\\\\user\\\\Desktop\\\\my-vishnu-proj'\\\\preprocessing\\\\drift\"\n",
    "#from importlib import reload \n",
    "#import drift_thresholder\n",
    "#from importlib import reload \n",
    "#reload(drift_thresholder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finisnhed hyperopt\n"
     ]
    }
   ],
   "source": [
    "print(\"finisnhed hyperopt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survived                     int64\n",
      "Pclass                       int64\n",
      "Age                        float64\n",
      "Siblings/Spouses Aboard      int64\n",
      "Parents/Children Aboard      int64\n",
      "Fare                       float64\n",
      "Sex_female                   uint8\n",
      "Sex_male                     uint8\n",
      "dtype: object\n",
      "['Pclass', 'Survived']\n"
     ]
    }
   ],
   "source": [
    "# first stage dropnana\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "data_up = data.drop([\"Name\"],axis=1)\n",
    "catgories_type_one_hot = [\"Sex\"]\n",
    "cat_type_label = [\"Pclass\",\"Survived\"]\n",
    "output_cat = [\"Survived\"]\n",
    "\n",
    "#data_up.drop([\"Name\"],axis=1)\n",
    "data_up=data_up.join(pd.get_dummies(data_up[catgories_type_one_hot]))\n",
    "data_up.drop([\"Sex\"],axis=1,inplace=True)\n",
    "#data_up.drop([\"sex\"],axis=1)\n",
    "data_up\n",
    "from sklearn.model_selection import train_test_split\n",
    "train=data_up.sample(frac=0.8,random_state=200) #random state is a seed value\n",
    "test=data_up.drop(train.index)\n",
    "X = data_up.drop([\"Survived\"],axis=1)\n",
    "y = data_up[\"Survived\"]\n",
    "#train_x,test_x,train_y,test_y = train_test_split(X,y,test_size=0.3)\n",
    "print(data_up.dtypes)\n",
    "print(cat_type_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders ={}\n",
    "filter_col1=[]\n",
    "for i in cat_type_label:\n",
    "    filter_col1.append(i)\n",
    "    lb = LabelEncoder().fit(data_up[i])\n",
    "    encoders[i]=lb\n",
    "    data_up[i]=lb.transform((data_up[i]))\n",
    "    \n",
    "filter_col1 = np.array(filter_col1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Pclass': LabelEncoder(), 'Survived': LabelEncoder()}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sex\n"
     ]
    }
   ],
   "source": [
    "filter_col=[]\n",
    "for j in catgories_type_one_hot:\n",
    "    print(j)\n",
    "    filter_col.append([col for col in data_up.columns if col.startswith(j+\"_\")])\n",
    "filter_col = np.array(filter_col).ravel() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cols =np.concatenate((filter_col1,filter_col),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Sex_female', 'Sex_male'], dtype='<U10')"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_dess ={}\n",
    "#for output \n",
    "for k in output_cat:\n",
    "    #print(k)\n",
    "    columns_dess[k]=\"output\"\n",
    "# for categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Survived': 'output'}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_dess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import auto_ml\n",
    "from auto_ml import Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Survived', 'Pclass', 'Age', 'Siblings/Spouses Aboard',\n",
       "       'Parents/Children Aboard', 'Fare', 'Sex_female', 'Sex_male'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_up.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to auto_ml! We're about to go through and make sense of your data using machine learning, and give you a production-ready pipeline to get predictions with.\n",
      "\n",
      "If you have any issues, or new feature ideas, let us know at http://auto.ml\n",
      "You are running on version 2.9.10\n",
      "Now using the model training_params that you passed in:\n",
      "{}\n",
      "After overwriting our defaults with your values, here are the final params that will be used to initialize the model:\n",
      "{'presort': False, 'learning_rate': 0.1, 'warm_start': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\base.py:197: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running basic data cleaning\n",
      "Fitting DataFrameVectorizer\n",
      "Now using the model training_params that you passed in:\n",
      "{}\n",
      "After overwriting our defaults with your values, here are the final params that will be used to initialize the model:\n",
      "{'presort': False, 'learning_rate': 0.1, 'warm_start': True}\n",
      "\n",
      "\n",
      "********************************************************************************************\n",
      "About to fit the pipeline for the model GradientBoostingClassifier to predict Survived\n",
      "Started at:\n",
      "2020-04-21 16:13:08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] random_holdout_set_from_training_data's score is: -0.217\n",
      "[2] random_holdout_set_from_training_data's score is: -0.2\n",
      "[3] random_holdout_set_from_training_data's score is: -0.185\n",
      "[4] random_holdout_set_from_training_data's score is: -0.175\n",
      "[5] random_holdout_set_from_training_data's score is: -0.165\n",
      "[6] random_holdout_set_from_training_data's score is: -0.157\n",
      "[7] random_holdout_set_from_training_data's score is: -0.151\n",
      "[8] random_holdout_set_from_training_data's score is: -0.147\n",
      "[9] random_holdout_set_from_training_data's score is: -0.143\n",
      "[10] random_holdout_set_from_training_data's score is: -0.139\n",
      "[11] random_holdout_set_from_training_data's score is: -0.138\n",
      "[12] random_holdout_set_from_training_data's score is: -0.136\n",
      "[13] random_holdout_set_from_training_data's score is: -0.135\n",
      "[14] random_holdout_set_from_training_data's score is: -0.134\n",
      "[15] random_holdout_set_from_training_data's score is: -0.132"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[16] random_holdout_set_from_training_data's score is: -0.133\n",
      "[17] random_holdout_set_from_training_data's score is: -0.132\n",
      "[18] random_holdout_set_from_training_data's score is: -0.132\n",
      "[19] random_holdout_set_from_training_data's score is: -0.132\n",
      "[20] random_holdout_set_from_training_data's score is: -0.131\n",
      "[21] random_holdout_set_from_training_data's score is: -0.132"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[22] random_holdout_set_from_training_data's score is: -0.132\n",
      "[23] random_holdout_set_from_training_data's score is: -0.132\n",
      "[24] random_holdout_set_from_training_data's score is: -0.132\n",
      "[25] random_holdout_set_from_training_data's score is: -0.131\n",
      "[26] random_holdout_set_from_training_data's score is: -0.132\n",
      "[27] random_holdout_set_from_training_data's score is: -0.132\n",
      "[28] random_holdout_set_from_training_data's score is: -0.132\n",
      "[29] random_holdout_set_from_training_data's score is: -0.133\n",
      "[30] random_holdout_set_from_training_data's score is: -0.132\n",
      "[31] random_holdout_set_from_training_data's score is: -0.132\n",
      "[32] random_holdout_set_from_training_data's score is: -0.132\n",
      "[33] random_holdout_set_from_training_data's score is: -0.131"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[34] random_holdout_set_from_training_data's score is: -0.131\n",
      "[35] random_holdout_set_from_training_data's score is: -0.132\n",
      "[36] random_holdout_set_from_training_data's score is: -0.131\n",
      "[37] random_holdout_set_from_training_data's score is: -0.131\n",
      "[38] random_holdout_set_from_training_data's score is: -0.131\n",
      "[39] random_holdout_set_from_training_data's score is: -0.13\n",
      "[40] random_holdout_set_from_training_data's score is: -0.131\n",
      "[41] random_holdout_set_from_training_data's score is: -0.13\n",
      "[42] random_holdout_set_from_training_data's score is: -0.13\n",
      "[43] random_holdout_set_from_training_data's score is: -0.131\n",
      "[44] random_holdout_set_from_training_data's score is: -0.13\n",
      "[45] random_holdout_set_from_training_data's score is: -0.13\n",
      "[46] random_holdout_set_from_training_data's score is: -0.13\n",
      "[47] random_holdout_set_from_training_data's score is: -0.131\n",
      "[48] random_holdout_set_from_training_data's score is: -0.131"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[49] random_holdout_set_from_training_data's score is: -0.13\n",
      "[50] random_holdout_set_from_training_data's score is: -0.13\n",
      "[52] random_holdout_set_from_training_data's score is: -0.131\n",
      "[54] random_holdout_set_from_training_data's score is: -0.131\n",
      "[56] random_holdout_set_from_training_data's score is: -0.132\n",
      "[58] random_holdout_set_from_training_data's score is: -0.132\n",
      "[60] random_holdout_set_from_training_data's score is: -0.131\n",
      "[62] random_holdout_set_from_training_data's score is: -0.131\n",
      "[64] random_holdout_set_from_training_data's score is: -0.131\n",
      "[66] random_holdout_set_from_training_data's score is: -0.132\n",
      "[68] random_holdout_set_from_training_data's score is: -0.131\n",
      "[70] random_holdout_set_from_training_data's score is: -0.132\n",
      "[72] random_holdout_set_from_training_data's score is: -0.133\n",
      "[74] random_holdout_set_from_training_data's score is: -0.133\n",
      "[76] random_holdout_set_from_training_data's score is: -0.133\n",
      "[78] random_holdout_set_from_training_data's score is: -0.134"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1342: FutureWarning: The parameter 'presort' is deprecated and has no effect. It will be removed in v0.24. You can suppress this warning by not passing any value to the 'presort' parameter. We also recommend using HistGradientBoosting models instead.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The number of estimators that were the best for this training dataset: 44\n",
      "The best score on the holdout set: -0.12977383122341427\n",
      "Finished training the pipeline!\n",
      "Total training time:\n",
      "0:00:08\n",
      "\n",
      "\n",
      "Here are the results from our GradientBoostingClassifier\n",
      "predicting Survived\n",
      "Calculating feature responses, for advanced analytics.\n",
      "The printed list will only contain at most the top 100 features.\n",
      "+----+-------------------------+--------------+---------+-------------------+-------------------+-----------+-----------+-----------+-----------+\n",
      "|    | Feature Name            |   Importance |   Delta |   FR_Decrementing |   FR_Incrementing |   FRD_abs |   FRI_abs |   FRD_MAD |   FRI_MAD |\n",
      "|----+-------------------------+--------------+---------+-------------------+-------------------+-----------+-----------+-----------+-----------|\n",
      "|  3 | Parents/Children Aboard |       0.0012 |  0.3903 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "|  2 | Siblings/Spouses Aboard |       0.0716 |  0.4782 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "|  0 | Pclass                  |       0.1106 |  0.4218 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "|  6 | Sex_male                |       0.1226 |  0.2382 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "|  1 | Age                     |       0.1630 |  7.1061 |            0.0415 |           -0.0226 |    0.0607 |    0.0458 |    0.0216 |    0.0206 |\n",
      "|  4 | Fare                    |       0.1665 | 26.0389 |           -0.0736 |            0.0477 |    0.0844 |    0.0790 |    0.0182 |    0.0779 |\n",
      "|  5 | Sex_female              |       0.3645 |  0.2382 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "+----+-------------------------+--------------+---------+-------------------+-------------------+-----------+-----------+-----------+-----------+\n",
      "\n",
      "\n",
      "*******\n",
      "Legend:\n",
      "Importance = Feature Importance\n",
      "     Explanation: A weighted measure of how much of the variance the model is able to explain is due to this column\n",
      "FR_delta = Feature Response Delta Amount\n",
      "     Explanation: Amount this column was incremented or decremented by to calculate the feature reponses\n",
      "FR_Decrementing = Feature Response From Decrementing Values In This Column By One FR_delta\n",
      "     Explanation: Represents how much the predicted output values respond to subtracting one FR_delta amount from every value in this column\n",
      "FR_Incrementing = Feature Response From Incrementing Values In This Column By One FR_delta\n",
      "     Explanation: Represents how much the predicted output values respond to adding one FR_delta amount to every value in this column\n",
      "FRD_MAD = Feature Response From Decrementing- Median Absolute Delta\n",
      "     Explanation: Takes the absolute value of all changes in predictions, then takes the median of those. Useful for seeing if decrementing this feature provokes strong changes that are both positive and negative\n",
      "FRI_MAD = Feature Response From Incrementing- Median Absolute Delta\n",
      "     Explanation: Takes the absolute value of all changes in predictions, then takes the median of those. Useful for seeing if incrementing this feature provokes strong changes that are both positive and negative\n",
      "FRD_abs = Feature Response From Decrementing Avg Absolute Change\n",
      "     Explanation: What is the average absolute change in predicted output values to subtracting one FR_delta amount to every value in this column. Useful for seeing if output is sensitive to a feature, but not in a uniformly positive or negative way\n",
      "FRI_abs = Feature Response From Incrementing Avg Absolute Change\n",
      "     Explanation: What is the average absolute change in predicted output values to adding one FR_delta amount to every value in this column. Useful for seeing if output is sensitive to a feature, but not in a uniformly positive or negative way\n",
      "*******\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\base.py:197: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "\n",
    "mll_pred = Predictor(type_of_estimator='classifier', column_descriptions=columns_dess)\n",
    "xdd=mll_pred.train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'percent_rows': 0.1,\n",
       " 'min_rows': 10000,\n",
       " 'cols_to_ignore': [],\n",
       " 'file_name': 'auto_ml_analytics_results_Survived.csv',\n",
       " 'col_std_multiplier': 0.5}"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mll_pred.analytics_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GradientBoostingClassifier']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mll_pred.model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ss\n"
     ]
    }
   ],
   "source": [
    "print(\"ss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =xdd.trained_final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "res =xdd.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.88034497, 0.11965503],\n",
       "       [0.43582036, 0.56417964],\n",
       "       [0.86373404, 0.13626596],\n",
       "       [0.92161284, 0.07838716],\n",
       "       [0.88034497, 0.11965503],\n",
       "       [0.88034497, 0.11965503],\n",
       "       [0.03036962, 0.96963038],\n",
       "       [0.40619423, 0.59380577],\n",
       "       [0.07223888, 0.92776112],\n",
       "       [0.09742883, 0.90257117],\n",
       "       [0.59743413, 0.40256587],\n",
       "       [0.79120025, 0.20879975],\n",
       "       [0.86291571, 0.13708429],\n",
       "       [0.1189933 , 0.8810067 ],\n",
       "       [0.88034497, 0.11965503],\n",
       "       [0.38391751, 0.61608249],\n",
       "       [0.77336396, 0.22663604],\n",
       "       [0.12893091, 0.87106909],\n",
       "       [0.88034497, 0.11965503],\n",
       "       [0.88366837, 0.11633163],\n",
       "       [0.14531018, 0.85468982],\n",
       "       [0.86291571, 0.13708429],\n",
       "       [0.50333172, 0.49666828],\n",
       "       [0.70865836, 0.29134164],\n",
       "       [0.88589029, 0.11410971],\n",
       "       [0.84524505, 0.15475495],\n",
       "       [0.08520533, 0.91479467],\n",
       "       [0.88034497, 0.11965503],\n",
       "       [0.77417796, 0.22582204],\n",
       "       [0.88034497, 0.11965503],\n",
       "       [0.40619423, 0.59380577],\n",
       "       [0.94678674, 0.05321326],\n",
       "       [0.88589029, 0.11410971],\n",
       "       [0.17885693, 0.82114307],\n",
       "       [0.45626238, 0.54373762],\n",
       "       [0.91750221, 0.08249779],\n",
       "       [0.87374408, 0.12625592],\n",
       "       [0.68783113, 0.31216887],\n",
       "       [0.55171108, 0.44828892],\n",
       "       [0.88589029, 0.11410971],\n",
       "       [0.87374408, 0.12625592],\n",
       "       [0.16259201, 0.83740799],\n",
       "       [0.88034497, 0.11965503],\n",
       "       [0.1189933 , 0.8810067 ],\n",
       "       [0.1010367 , 0.8989633 ],\n",
       "       [0.86291571, 0.13708429],\n",
       "       [0.85671024, 0.14328976],\n",
       "       [0.87374408, 0.12625592],\n",
       "       [0.86751952, 0.13248048],\n",
       "       [0.79584846, 0.20415154],\n",
       "       [0.86291571, 0.13708429],\n",
       "       [0.88034497, 0.11965503],\n",
       "       [0.81066562, 0.18933438],\n",
       "       [0.87374408, 0.12625592],\n",
       "       [0.88034497, 0.11965503],\n",
       "       [0.86291571, 0.13708429],\n",
       "       [0.53672584, 0.46327416],\n",
       "       [0.92763754, 0.07236246],\n",
       "       [0.07223888, 0.92776112],\n",
       "       [0.87374408, 0.12625592],\n",
       "       [0.38391751, 0.61608249],\n",
       "       [0.86291571, 0.13708429],\n",
       "       [0.88034497, 0.11965503],\n",
       "       [0.94226608, 0.05773392],\n",
       "       [0.86291571, 0.13708429],\n",
       "       [0.40619423, 0.59380577],\n",
       "       [0.84643641, 0.15356359],\n",
       "       [0.86280272, 0.13719728],\n",
       "       [0.93606117, 0.06393883],\n",
       "       [0.07223888, 0.92776112],\n",
       "       [0.54192752, 0.45807248],\n",
       "       [0.06601369, 0.93398631],\n",
       "       [0.86291571, 0.13708429],\n",
       "       [0.1420896 , 0.8579104 ],\n",
       "       [0.54192752, 0.45807248],\n",
       "       [0.65811402, 0.34188598],\n",
       "       [0.86751952, 0.13248048],\n",
       "       [0.86291571, 0.13708429],\n",
       "       [0.85019037, 0.14980963],\n",
       "       [0.86291571, 0.13708429],\n",
       "       [0.13043184, 0.86956816],\n",
       "       [0.39329769, 0.60670231],\n",
       "       [0.88034497, 0.11965503],\n",
       "       [0.70555121, 0.29444879],\n",
       "       [0.08308371, 0.91691629],\n",
       "       [0.15290599, 0.84709401],\n",
       "       [0.88034497, 0.11965503],\n",
       "       [0.86751952, 0.13248048],\n",
       "       [0.88034497, 0.11965503],\n",
       "       [0.88589029, 0.11410971],\n",
       "       [0.42613835, 0.57386165],\n",
       "       [0.86291571, 0.13708429],\n",
       "       [0.38014792, 0.61985208],\n",
       "       [0.67519471, 0.32480529],\n",
       "       [0.10801596, 0.89198404],\n",
       "       [0.88034497, 0.11965503],\n",
       "       [0.86291571, 0.13708429],\n",
       "       [0.88589029, 0.11410971],\n",
       "       [0.88589029, 0.11410971],\n",
       "       [0.29819814, 0.70180186],\n",
       "       [0.43582036, 0.56417964],\n",
       "       [0.81359545, 0.18640455],\n",
       "       [0.40619423, 0.59380577],\n",
       "       [0.70341665, 0.29658335],\n",
       "       [0.65811402, 0.34188598],\n",
       "       [0.16259201, 0.83740799],\n",
       "       [0.09742883, 0.90257117],\n",
       "       [0.87374408, 0.12625592],\n",
       "       [0.88034497, 0.11965503],\n",
       "       [0.80198695, 0.19801305],\n",
       "       [0.88034497, 0.11965503],\n",
       "       [0.88034497, 0.11965503],\n",
       "       [0.86291571, 0.13708429],\n",
       "       [0.88589029, 0.11410971],\n",
       "       [0.42613835, 0.57386165],\n",
       "       [0.9490803 , 0.0509197 ],\n",
       "       [0.16259201, 0.83740799],\n",
       "       [0.88589029, 0.11410971],\n",
       "       [0.39618265, 0.60381735],\n",
       "       [0.10095632, 0.89904368],\n",
       "       [0.67888773, 0.32111227],\n",
       "       [0.87374408, 0.12625592],\n",
       "       [0.85019037, 0.14980963],\n",
       "       [0.86751952, 0.13248048],\n",
       "       [0.79294934, 0.20705066],\n",
       "       [0.82748048, 0.17251952],\n",
       "       [0.07329133, 0.92670867],\n",
       "       [0.29819814, 0.70180186],\n",
       "       [0.88366837, 0.11633163],\n",
       "       [0.13648588, 0.86351412],\n",
       "       [0.88034497, 0.11965503],\n",
       "       [0.86751952, 0.13248048],\n",
       "       [0.88589029, 0.11410971],\n",
       "       [0.86291571, 0.13708429],\n",
       "       [0.86751952, 0.13248048],\n",
       "       [0.88034497, 0.11965503],\n",
       "       [0.88589029, 0.11410971],\n",
       "       [0.53672584, 0.46327416],\n",
       "       [0.67888773, 0.32111227],\n",
       "       [0.17885693, 0.82114307],\n",
       "       [0.70341665, 0.29658335],\n",
       "       [0.08614139, 0.91385861],\n",
       "       [0.70341665, 0.29658335],\n",
       "       [0.88034497, 0.11965503],\n",
       "       [0.85149842, 0.14850158],\n",
       "       [0.87709656, 0.12290344],\n",
       "       [0.61457195, 0.38542805],\n",
       "       [0.88034497, 0.11965503],\n",
       "       [0.05576553, 0.94423447],\n",
       "       [0.07065451, 0.92934549],\n",
       "       [0.88589029, 0.11410971],\n",
       "       [0.86751952, 0.13248048],\n",
       "       [0.1189933 , 0.8810067 ],\n",
       "       [0.88034497, 0.11965503],\n",
       "       [0.785069  , 0.214931  ],\n",
       "       [0.61457195, 0.38542805],\n",
       "       [0.86291571, 0.13708429],\n",
       "       [0.40619423, 0.59380577],\n",
       "       [0.79009976, 0.20990024],\n",
       "       [0.86291571, 0.13708429],\n",
       "       [0.88589029, 0.11410971],\n",
       "       [0.40101196, 0.59898804],\n",
       "       [0.49514545, 0.50485455],\n",
       "       [0.50333172, 0.49666828],\n",
       "       [0.86291571, 0.13708429],\n",
       "       [0.67519471, 0.32480529],\n",
       "       [0.88034497, 0.11965503],\n",
       "       [0.85671024, 0.14328976],\n",
       "       [0.74486771, 0.25513229],\n",
       "       [0.10801596, 0.89198404],\n",
       "       [0.07223888, 0.92776112],\n",
       "       [0.785069  , 0.214931  ],\n",
       "       [0.86751952, 0.13248048],\n",
       "       [0.08615477, 0.91384523],\n",
       "       [0.43582036, 0.56417964],\n",
       "       [0.85844313, 0.14155687],\n",
       "       [0.40266345, 0.59733655]])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xdd.predict_proba(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is our brier-score-loss, which is the default value we optimized for while training, and is the value returned from .score() unless you requested a custom scoring metric\n",
      "It is a measure of how close the PROBABILITY predictions are.\n",
      "0.1282\n",
      "\n",
      "Here is the trained estimator's overall accuracy (when it predicts a label, how frequently is that the correct label?)\n",
      "82.5%\n",
      "\n",
      "Here is a confusion matrix showing predictions vs. actuals by label:\n",
      "Predicted >    0   1  All\n",
      "v Actual v               \n",
      "0            100  11  111\n",
      "1             20  46   66\n",
      "All          120  57  177\n",
      "\n",
      "Here is predictive value by class:\n",
      "Class:  0 = 0.8333333333333334\n",
      "Class:  1 = 0.8070175438596491\n",
      "+-----------------+-----------------------------------+--------------------------------+\n",
      "| Bucket Edges    |   Predicted Probability Of Bucket |   Actual Probability of Bucket |\n",
      "|-----------------+-----------------------------------+--------------------------------|\n",
      "| (0.04, 0.0962]  |                            0.0822 |                         0.2778 |\n",
      "| (0.0962, 0.108] |                            0.1004 |                         0.1111 |\n",
      "| (0.108, 0.118]  |                            0.1127 |                         0.1176 |\n",
      "| (0.118, 0.127]  |                            0.1221 |                         0.0909 |\n",
      "| (0.127, 0.168]  |                            0.1421 |                         0.0000 |\n",
      "| (0.168, 0.339]  |                            0.2694 |                         0.1111 |\n",
      "| (0.339, 0.531]  |                            0.4614 |                         0.4706 |\n",
      "| (0.531, 0.744]  |                            0.6222 |                         0.6471 |\n",
      "| (0.744, 0.88]   |                            0.8272 |                         0.8947 |\n",
      "| (0.88, 0.952]   |                            0.9196 |                         1.0000 |\n",
      "+-----------------+-----------------------------------+--------------------------------+\n",
      "\n",
      "Here is the accuracy of our trained estimator at each level of predicted probabilities\n",
      "For a verbose description of what this means, please visit the docs:\n",
      "http://auto-ml.readthedocs.io/en/latest/analytics.html#interpreting-predicted-probability-buckets-for-classifiers\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.1281760743437837"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xdd.score(test.drop([\"Survived\"],axis=1),test[\"Survived\"],advanced_scoring=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn\n",
    "sklearn.metrics.acuracy_score(test[\"Survived\"],res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.807909604519774"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.accuracy_score(test[\"Survived\"],res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdd.scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlbox"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: auto-ml 2.9.10 has requirement lightgbm<2.1,>=2.0.11, but you'll have lightgbm 2.3.1 which is incompatible.\n",
      "ERROR: Could not install packages due to an EnvironmentError: [WinError 5] Access is denied: 'c:\\\\users\\\\user\\\\appdata\\\\local\\\\programs\\\\python\\\\python37\\\\lib\\\\site-packages\\\\~atplotlib\\\\ft2font.cp37-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading mlbox-0.8.4.tar.gz (31 kB)\n",
      "Requirement already satisfied: numpy==1.18.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from mlbox) (1.18.2)\n",
      "Requirement already satisfied: scipy==1.4.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from mlbox) (1.4.1)\n",
      "Collecting matplotlib==3.0.3\n",
      "  Downloading matplotlib-3.0.3-cp37-cp37m-win_amd64.whl (9.1 MB)\n",
      "Collecting hyperopt==0.2.3\n",
      "  Downloading hyperopt-0.2.3-py3-none-any.whl (1.9 MB)\n",
      "Requirement already satisfied: pandas==0.25.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from mlbox) (0.25.3)\n",
      "Requirement already satisfied: joblib==0.14.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from mlbox) (0.14.1)\n",
      "Collecting scikit-learn==0.22.1\n",
      "  Downloading scikit_learn-0.22.1-cp37-cp37m-win_amd64.whl (6.3 MB)\n",
      "Collecting tensorflow==2.0.1\n",
      "  Downloading tensorflow-2.0.1-cp37-cp37m-win_amd64.whl (48.1 MB)\n",
      "Collecting lightgbm==2.3.1\n",
      "  Downloading lightgbm-2.3.1-py2.py3-none-win_amd64.whl (544 kB)\n",
      "Collecting tables==3.5.2\n",
      "  Downloading tables-3.5.2-cp37-cp37m-win_amd64.whl (3.2 MB)\n",
      "Collecting xlrd==1.2.0\n",
      "  Downloading xlrd-1.2.0-py2.py3-none-any.whl (103 kB)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib==3.0.3->mlbox) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib==3.0.3->mlbox) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib==3.0.3->mlbox) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib==3.0.3->mlbox) (1.2.0)\n",
      "Collecting future\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from hyperopt==0.2.3->mlbox) (4.45.0)\n",
      "Requirement already satisfied: six in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from hyperopt==0.2.3->mlbox) (1.14.0)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from hyperopt==0.2.3->mlbox) (1.3.0)\n",
      "Collecting networkx==2.2\n",
      "  Downloading networkx-2.2.zip (1.7 MB)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas==0.25.3->mlbox) (2019.3)\n",
      "Collecting wheel>=0.26; python_version >= \"3\"\n",
      "  Downloading wheel-0.34.2-py2.py3-none-any.whl (26 kB)\n",
      "Processing c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\cc\\af\\1a\\498a24d0730ef484019e007bb9e8cef3ac00311a672c049a3e\\absl_py-0.9.0-py3-none-any.whl\n",
      "Collecting astor>=0.6.0\n",
      "  Using cached astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Collecting keras-applications>=1.0.8\n",
      "  Using cached Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
      "Collecting keras-preprocessing>=1.0.5\n",
      "  Using cached Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41 kB)\n",
      "Collecting protobuf>=3.6.1\n",
      "  Using cached protobuf-3.11.3-cp37-cp37m-win_amd64.whl (1.0 MB)\n",
      "Collecting tensorboard<2.1.0,>=2.0.0\n",
      "  Downloading tensorboard-2.0.2-py3-none-any.whl (3.8 MB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.2.1-py3-none-any.whl (63 kB)\n",
      "Processing c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\21\\7f\\02\\420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\\gast-0.2.2-py3-none-any.whl\n",
      "Collecting tensorflow-estimator<2.1.0,>=2.0.0\n",
      "  Downloading tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449 kB)\n",
      "Collecting google-pasta>=0.1.6\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow==2.0.1->mlbox) (1.11.2)\n",
      "Processing c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\3f\\e3\\ec\\8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\\termcolor-1.1.0-py3-none-any.whl\n",
      "Collecting grpcio>=1.8.6\n",
      "  Using cached grpcio-1.28.1-cp37-cp37m-win_amd64.whl (2.0 MB)\n",
      "Collecting mock>=2.0\n",
      "  Downloading mock-4.0.2-py3-none-any.whl (28 kB)\n",
      "Collecting numexpr>=2.6.2\n",
      "  Downloading numexpr-2.7.1-cp37-none-win_amd64.whl (90 kB)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from networkx==2.2->hyperopt==0.2.3->mlbox) (4.4.2)\n",
      "Requirement already satisfied: h5py in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from keras-applications>=1.0.8->tensorflow==2.0.1->mlbox) (2.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from protobuf>=3.6.1->tensorflow==2.0.1->mlbox) (41.2.0)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.2.1-py2.py3-none-any.whl (88 kB)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Using cached google_auth-1.14.0-py2.py3-none-any.whl (88 kB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Using cached Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.1->mlbox) (2.23.0)\n",
      "Collecting rsa<4.1,>=3.1.4\n",
      "  Using cached rsa-4.0-py2.py3-none-any.whl (38 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Using cached cachetools-4.1.0-py3-none-any.whl (10 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.1->mlbox) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.1->mlbox) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.1->mlbox) (2020.4.5.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.1->mlbox) (1.25.9)\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Installing collected packages: matplotlib, future, networkx, hyperopt, scikit-learn, wheel, absl-py, astor, keras-applications, keras-preprocessing, protobuf, markdown, grpcio, pyasn1, rsa, cachetools, pyasn1-modules, google-auth, werkzeug, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard, opt-einsum, gast, tensorflow-estimator, google-pasta, termcolor, tensorflow, lightgbm, mock, numexpr, tables, xlrd, mlbox\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.2.1\n",
      "    Uninstalling matplotlib-3.2.1:\n",
      "      Successfully uninstalled matplotlib-3.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install mlbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yoyo\n"
     ]
    }
   ],
   "source": [
    "print(\"yoyo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_predict\n",
    "\n",
    "class DriftEstimator():\n",
    "\n",
    "    \"\"\"Estimates the drift between two datasets\n",
    "    \n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : classifier, defaut = RandomForestClassifier(n_estimators = 50, n_jobs=-1, max_features=1., min_samples_leaf = 5, max_depth = 5)\n",
    "        The estimator that estimates the drift between two datasets\n",
    "        \n",
    "    n_folds : int, defaut = 2\n",
    "        Number of folds used to estimate the drift\n",
    "\n",
    "    stratify : bool, defaut = True\n",
    "        Whether the cv is stratified (same number of train and test samples within each fold)\n",
    "\n",
    "    random_state : int, defaut = 1\n",
    "        Random state for cv\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 estimator=RandomForestClassifier(n_estimators=50,\n",
    "                                                  n_jobs=-1,\n",
    "                                                  max_features=1.,\n",
    "                                                  min_samples_leaf=5,\n",
    "                                                  max_depth=5),\n",
    "                 n_folds=2,\n",
    "                 stratify=True,\n",
    "                 random_state=1):\n",
    "\n",
    "        self.estimator = estimator\n",
    "        self.n_folds = n_folds\n",
    "        self.stratify = stratify\n",
    "        self.random_state = random_state\n",
    "        self.__cv = None\n",
    "        self.__pred = None\n",
    "        self.__target = None\n",
    "        self.__fitOK = False\n",
    "\n",
    "    def get_params(self):\n",
    "\n",
    "        return {'estimator': self.estimator,\n",
    "                'n_folds': self.n_folds,\n",
    "                'stratify': self.stratify,\n",
    "                'random_state': self.random_state}\n",
    "\n",
    "    def set_params(self, **params):\n",
    "\n",
    "        if('estimator' in params.keys()):\n",
    "            self.estimator = params['estimator']\n",
    "        if('n_folds' in params.keys()):\n",
    "            self.n_folds = params['n_folds']\n",
    "        if('stratify' in params.keys()):\n",
    "            self.stratify = params['stratify']\n",
    "        if('random_state' in params.keys()):\n",
    "            self.random_state = params['random_state']\n",
    "\n",
    "    def fit(self, df_train, df_test):\n",
    "\n",
    "        \"\"\"\n",
    "        Computes the drift between the two datasets\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df_train : pandas dataframe of shape = (n_train, p)\n",
    "            The train set\n",
    "\n",
    "        df_test : pandas dataframe of shape = (n_test, p)\n",
    "            The test set\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "\n",
    "        df_train[\"target\"] = 0\n",
    "        df_test[\"target\"] = 1\n",
    "\n",
    "        self.__target = pd.concat((df_train.target, df_test.target),\n",
    "                                  ignore_index=True)\n",
    "\n",
    "        if self.stratify:\n",
    "            self.__cv = StratifiedKFold(n_splits=self.n_folds,\n",
    "                                        shuffle=True,\n",
    "                                        random_state=self.random_state)\n",
    "        else:\n",
    "            self.__cv = KFold(n_splits=self.n_folds,\n",
    "                              shuffle=True,\n",
    "                              random_state=self.random_state)\n",
    "\n",
    "        X_tmp = pd.concat((df_train, df_test),\n",
    "                          ignore_index=True).drop(['target'], axis=1)\n",
    "\n",
    "        self.__pred = cross_val_predict(estimator=self.estimator,\n",
    "                                        X=X_tmp,\n",
    "                                        y=self.__target,\n",
    "                                        cv=self.__cv,\n",
    "                                        method=\"predict_proba\")[:,1]\n",
    "\n",
    "        del df_train[\"target\"]\n",
    "        del df_test[\"target\"]\n",
    "\n",
    "        self.__fitOK = True\n",
    "\n",
    "        return self\n",
    "\n",
    "    def score(self):\n",
    "        \n",
    "        \"\"\"Returns the global drift measure between two datasets.\n",
    "\n",
    "         0. = No drift. 1. = Maximal Drift\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The drift measure\n",
    "        \"\"\"\n",
    "\n",
    "        S = []\n",
    "\n",
    "        if self.__fitOK:\n",
    "\n",
    "            X_zeros = np.zeros(len(self.__target))\n",
    "\n",
    "            for train_index, test_index in self.__cv.split(X=X_zeros,\n",
    "                                                           y=self.__target):\n",
    "\n",
    "                S.append(roc_auc_score(self.__target.iloc[test_index],\n",
    "                                       self.__pred[test_index]))\n",
    "\n",
    "            return (max(np.mean(S), 1-np.mean(S))-0.5) * 2\n",
    "\n",
    "        else:\n",
    "            raise ValueError('Call the fit function before !')\n",
    "\n",
    "    def predict(self):\n",
    "\n",
    "        \"\"\"Returns the probabilities that the sample belongs to the test dataset\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Array of shape = (n_train+n_test,)\n",
    "            The probabilities\n",
    "        \"\"\"\n",
    "\n",
    "        if self.__fitOK:\n",
    "\n",
    "            return self.__pred\n",
    "\n",
    "        else:\n",
    "            raise ValueError('Call the fit function before !')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "# Authors: Axel ARONIO DE ROMBLAY <axelderomblay@gmail.com>\n",
    "#          Alexis BONDU <alexis.bondu@gmail.com>\n",
    "# License: BSD 3 clause\n",
    "import sys\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#from drift_estimator import DriftEstimator\n",
    "DriftEstimator = DriftEstimator()\n",
    "\n",
    "def sync_fit(df_train, df_test, estimator, n_folds=2, stratify=True, random_state=1):\n",
    "    \"\"\"Compute the univariate drifts between df_train and df_test datasets.\n",
    "\n",
    "    Multi-threaded version.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_train : pandas dataframe of shape = (n_train, p)\n",
    "        The train set\n",
    "\n",
    "    df_test : pandas dataframe of shape = (n_test, p)\n",
    "        The test set\n",
    "\n",
    "    estimator : classifier, defaut = RandomForestClassifier(n_estimators = 50,\n",
    "                                                            n_jobs=-1,\n",
    "                                                            max_features=1.,\n",
    "                                                            min_samples_leaf = 5,\n",
    "                                                            max_depth = 5)\n",
    "        The estimator that estimates the drift between two datasets\n",
    "\n",
    "    n_folds : int, default = 2\n",
    "        Number of folds used to estimate the drift\n",
    "\n",
    "    stratify : bool, default = True\n",
    "        Whether the cv is stratified (same number of train and test samples\n",
    "        within each fold)\n",
    "\n",
    "    random_state : int, default = 1\n",
    "        Random state for cv\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        drift measure\n",
    "\n",
    "    \"\"\"\n",
    "    # We will compute the indices of the CV in each thread\n",
    "    de = DriftEstimator(estimator, n_folds, stratify, random_state)\n",
    "    de.fit(df_train, df_test)\n",
    "\n",
    "    return de.score()\n",
    "\n",
    "\n",
    "class DriftThreshold():\n",
    "    \"\"\"Estimate the univariate drift between two datasets.\n",
    "\n",
    "    Estimate the univariate drift between two datasets\n",
    "    and select features with low drifts\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    threshold : float, defaut = 0.6\n",
    "        The drift threshold (univariate drift below are kept)\n",
    "        Must be between 0. and 1.\n",
    "\n",
    "    subsample : float, defaut = 1.\n",
    "        Subsampling parameter for the datasets.\n",
    "        Must be between 0. and 1.\n",
    "\n",
    "    estimator : classifier, default = DecisionTreeClassifier(max_depth=6)\n",
    "        The estimator that estimates the drift between two datasets.\n",
    "\n",
    "    n_folds : int, default = 2\n",
    "        Number of folds used to estimate the drift.\n",
    "\n",
    "    stratify : bool, default = True\n",
    "        Whether the cv is stratified (same number of train and test samples\n",
    "        within each fold)\n",
    "\n",
    "    random_state : int, default = 1\n",
    "        Seed for for cv and subsampling.\n",
    "\n",
    "    n_jobs : int, defaut = -1\n",
    "        Number of cores used for processing (-1 for all cores)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 threshold=0.6,\n",
    "                 subsample=1.,\n",
    "                 estimator=DecisionTreeClassifier(max_depth=6),\n",
    "                 n_folds=2,\n",
    "                 stratify=True,\n",
    "                 random_state=1,\n",
    "                 n_jobs=-1):\n",
    "        \"\"\"Init a DriftThreshold object.\"\"\"\n",
    "        self.threshold = threshold\n",
    "        self.subsample = subsample\n",
    "        self.estimator = estimator\n",
    "        self.n_folds = n_folds\n",
    "        self.stratify = stratify\n",
    "        self.random_state = random_state\n",
    "        self.n_jobs = n_jobs\n",
    "        self.__Ddrifts = dict()\n",
    "        self.__fitOK = False\n",
    "\n",
    "    def get_params(self):\n",
    "        \"\"\"Get parameters of a DriftThreshold object.\"\"\"\n",
    "        return {'threshold': self.threshold,\n",
    "                'subsample': self.subsample,\n",
    "                'estimator': self.estimator,\n",
    "                'n_folds': self.n_folds,\n",
    "                'stratify': self.stratify,\n",
    "                'random_state': self.random_state,\n",
    "                'n_jobs': self.n_jobs}\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        \"\"\"Set parameters of a DriftThreshold object.\"\"\"\n",
    "        if('threshold' in params.keys()):\n",
    "            self.threshold = params['threshold']\n",
    "        if('subsample' in params.keys()):\n",
    "            self.subsample = params['subsample']\n",
    "        if('estimator' in params.keys()):\n",
    "            self.estimator = params['estimator']\n",
    "        if('n_folds' in params.keys()):\n",
    "            self.n_folds = params['n_folds']\n",
    "        if('stratify' in params.keys()):\n",
    "            self.stratify = params['stratify']\n",
    "        if('random_state' in params.keys()):\n",
    "            self.random_state = params['random_state']\n",
    "        if('n_jobs' in params.keys()):\n",
    "            self.n_jobs = params['n_jobs']\n",
    "\n",
    "    def fit(self, df_train, df_test):\n",
    "        \"\"\"Compute the univariate drifts between df_train and df_test datasets.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df_train : pandas dataframe of shape = (n_train, p)\n",
    "            The train set\n",
    "\n",
    "        df_test : pandas dataframe of shape = (n_test, p)\n",
    "            The test set\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "\n",
    "        \"\"\"\n",
    "        self.__Ddrifts = dict()\n",
    "\n",
    "        if sys.platform == 'win32':\n",
    "            Ldrifts = [sync_fit(df_train.sample(frac=self.subsample)[[col]],\n",
    "                               df_test.sample(frac=self.subsample)[[col]],\n",
    "                               self.estimator,\n",
    "                               self.n_folds,\n",
    "                               self.stratify,\n",
    "                               self.random_state)\n",
    "                               for col in df_train.columns]\n",
    "        else:\n",
    "            Ldrifts = Parallel(n_jobs=self.n_jobs)(delayed(sync_fit)\n",
    "                                               (df_train.sample(frac=self.subsample)[[col]],\n",
    "                                                df_test.sample(frac=self.subsample)[[col]],\n",
    "                                                self.estimator,\n",
    "                                                self.n_folds,\n",
    "                                                self.stratify,\n",
    "                                                self.random_state)\n",
    "                                               for col in df_train.columns)\n",
    "\n",
    "        for i, col in enumerate(df_train.columns):\n",
    "\n",
    "            self.__Ddrifts[col] = Ldrifts[i]\n",
    "\n",
    "        del Ldrifts\n",
    "\n",
    "        self.__fitOK = True\n",
    "\n",
    "    def transform(self, df):\n",
    "        \"\"\"Select the features with low drift.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas dataframe\n",
    "            A dataset with the same features\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pandas DataFrame\n",
    "            The transformed dataframe\n",
    "\n",
    "        \"\"\"\n",
    "        if self.__fitOK:\n",
    "\n",
    "            selected_col = []\n",
    "\n",
    "            for i, col in enumerate(df.columns):\n",
    "\n",
    "                if (self.__Ddrifts[col] < self.threshold):\n",
    "                    selected_col.append(col)\n",
    "\n",
    "            return df[selected_col]\n",
    "\n",
    "        else:\n",
    "            raise ValueError('Call the fit function before !')\n",
    "\n",
    "    def get_support(self, complement=False):\n",
    "        \"\"\"Return the variables kept or dropped.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        complement : bool, default = True\n",
    "            If True, returns the features to drop\n",
    "            If False, returns the features to keep\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            The list of features to keep or to drop.\n",
    "\n",
    "        \"\"\"\n",
    "        if self.__fitOK:\n",
    "\n",
    "            keepList = []\n",
    "            dropList = []\n",
    "\n",
    "            for col in self.__Ddrifts:\n",
    "\n",
    "                if (self.__Ddrifts[col] < self.threshold):\n",
    "                    keepList.append(col)\n",
    "                else:\n",
    "                    dropList.append(col)\n",
    "\n",
    "            if complement:\n",
    "                return dropList\n",
    "            else:\n",
    "                return keepList\n",
    "        else:\n",
    "            raise ValueError('Call the fit function before !')\n",
    "\n",
    "    def drifts(self):\n",
    "        \"\"\"Return the univariate drifts for all variables.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            The dictionnary of drift measures for each features\n",
    "\n",
    "        \"\"\"\n",
    "        if self.__fitOK:\n",
    "\n",
    "            return self.__Ddrifts\n",
    "\n",
    "        else:\n",
    "            raise ValueError('Call the fit function before !')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-7e361ff4afc9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mReshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "#categorical encoder\n",
    "# coding: utf-8\n",
    "# Author: Axel ARONIO DE ROMBLAY <axelderomblay@gmail.com>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Reshape, Dropout, Embedding, concatenate, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "class Categorical_encoder():\n",
    "    \"\"\"Encodes categorical features.\n",
    "\n",
    "    Several strategies are possible (supervised or not). Works for both\n",
    "    classification and regression tasks.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    strategy : str, default = \"label_encoding\"\n",
    "        The strategy to encode categorical features.\n",
    "        Available strategies = {\"label_encoding\", \"dummification\",\n",
    "        \"random_projection\", entity_embedding\"}\n",
    "    verbose : bool, default = False\n",
    "        Verbose mode. Useful for entity embedding strategy.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, strategy='label_encoding', verbose=False):\n",
    "        \"\"\"Init method for class Categorical_encoder().\"\"\"\n",
    "        self.strategy = strategy\n",
    "        self.verbose = verbose\n",
    "        self.__Lcat = []\n",
    "        self.__Lnum = []\n",
    "        self.__Enc = dict()\n",
    "        self.__K = dict()\n",
    "        self.__weights = None\n",
    "        self.__fitOK = False\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        \"\"\"Get param that can be defined by the user.\n",
    "\n",
    "        Get strategy parameters and verbose parameters\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        strategy : str, default = \"label_encoding\"\n",
    "            The strategy to encode categorical features.\n",
    "            Available strategies = {\"label_encoding\", \"dummification\",\n",
    "            \"random_projection\", entity_embedding\"}\n",
    "        verbose : bool, default = False\n",
    "            Verbose mode. Useful for entity embedding strategy.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict : dictionary\n",
    "            Dictionary that contains strategy and verbose parameters.\n",
    "\n",
    "        \"\"\"\n",
    "        dict = {'strategy': self.strategy,\n",
    "                'verbose': self.verbose}\n",
    "        return dict\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        \"\"\"Set param method for Categorical encoder.\n",
    "\n",
    "        Set strategy parameters and verbose parameters\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        strategy : str, default = \"label_encoding\"\n",
    "            The strategy to encode categorical features.\n",
    "            Available strategies = {\"label_encoding\", \"dummification\",\n",
    "            \"random_projection\", entity_embedding\"}\n",
    "        verbose : bool, default = False\n",
    "            Verbose mode. Useful for entity embedding strategy.\n",
    "\n",
    "        \"\"\"\n",
    "        self.__fitOK = False\n",
    "\n",
    "        for k, v in params.items():\n",
    "            if k not in self.get_params():\n",
    "                warnings.warn(\"Invalid parameter(s) for encoder \"\n",
    "                              \"Categorical_encoder. Parameter(s) IGNORED. \"\n",
    "                              \"Check the list of available parameters with \"\n",
    "                              \"`encoder.get_params().keys()`\")\n",
    "            else:\n",
    "                setattr(self, k, v)\n",
    "\n",
    "    def fit(self, df_train, y_train):\n",
    "        \"\"\"Fit Categorical Encoder.\n",
    "\n",
    "        Encode categorical variable of a dataframe\n",
    "        following strategy parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df_train : pandas.Dataframe of shape = (n_train, n_features).\n",
    "            The training dataset with numerical and categorical features.\n",
    "            NA values are allowed.\n",
    "        y_train : pandas.Series of shape = (n_train, ).\n",
    "            The target for classification or regression tasks.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        object\n",
    "            self\n",
    "\n",
    "        \"\"\"\n",
    "        self.__Lcat = df_train.dtypes[df_train.dtypes == 'object'].index\n",
    "        self.__Lnum = df_train.dtypes[df_train.dtypes != 'object'].index\n",
    "\n",
    "        if (len(self.__Lcat) == 0):\n",
    "            self.__fitOK = True\n",
    "\n",
    "        else:\n",
    "\n",
    "            #################################################\n",
    "            #                Label Encoding\n",
    "            #################################################\n",
    "\n",
    "            if (self.strategy == 'label_encoding'):\n",
    "\n",
    "                for col in self.__Lcat:\n",
    "\n",
    "                    d = dict()\n",
    "                    levels = list(df_train[col].unique())\n",
    "                    nan = False\n",
    "\n",
    "                    if np.NaN in levels:\n",
    "                        nan = True\n",
    "                        levels.remove(np.NaN)\n",
    "\n",
    "                    for enc, level in enumerate([np.NaN]*nan + sorted(levels)):\n",
    "                        d[level] = enc  # TODO: Optimize loop?\n",
    "\n",
    "                    self.__Enc[col] = d\n",
    "\n",
    "                self.__fitOK = True\n",
    "\n",
    "            #################################################\n",
    "            #                Dummification\n",
    "            #################################################\n",
    "\n",
    "            elif (self.strategy == 'dummification'):\n",
    "\n",
    "                for col in self.__Lcat:\n",
    "                    # TODO: Optimize?\n",
    "                    self.__Enc[col] = list(df_train[col].dropna().unique())\n",
    "\n",
    "                self.__fitOK = True\n",
    "\n",
    "            #################################################\n",
    "            #                Entity Embedding\n",
    "            #################################################\n",
    "\n",
    "            elif (self.strategy == 'entity_embedding'):\n",
    "\n",
    "                # Parameters\n",
    "                A = 10   # 15 : more complex\n",
    "                B = 5    # 2 or 3 : more complex\n",
    "\n",
    "                # computing interactions\n",
    "                self.__K = {}\n",
    "                for col in self.__Lcat:\n",
    "                    exp_ = np.exp(-df_train[col].nunique() * 0.05)\n",
    "                    self.__K[col] = np.int(5 * (1 - exp_) + 1)\n",
    "\n",
    "                sum_ = sum([1. * np.log(k) for k in self.__K.values()])\n",
    "                # TODO: Add reference for this formula?\n",
    "\n",
    "                # Number of neurons for layer 1 and 2\n",
    "                n_layer1 = min(1000,\n",
    "                               int(A * (len(self.__K) ** 0.5) * sum_ + 1))\n",
    "                n_layer2 = int(n_layer1 / B) + 2\n",
    "\n",
    "                # Dropouts\n",
    "                dropout1 = 0.1\n",
    "                dropout2 = 0.1\n",
    "\n",
    "                # Learning parameters\n",
    "                epochs = 20  # 25 : more iterations\n",
    "                batch_size = 128  # 256 : gradient more stable\n",
    "\n",
    "                # Creating the neural network\n",
    "\n",
    "                embeddings = []\n",
    "                inputs = []\n",
    "\n",
    "                for col in self.__Lcat:\n",
    "\n",
    "                    d = dict()\n",
    "                    levels = list(df_train[col].unique())\n",
    "                    nan = False\n",
    "\n",
    "                    if np.NaN in levels:\n",
    "                        nan = True\n",
    "                        levels.remove(np.NaN)\n",
    "\n",
    "                    for enc, level in enumerate([np.NaN]*nan + sorted(levels)):\n",
    "                        d[level] = enc  # TODO: Optimize loop?\n",
    "\n",
    "                    self.__Enc[col] = d\n",
    "\n",
    "                    var = Input(shape=(1,))\n",
    "                    inputs.append(var)\n",
    "\n",
    "                    emb = Embedding(input_dim=len(self.__Enc[col]),\n",
    "                                    output_dim=self.__K[col],\n",
    "                                    input_length=1)(var)\n",
    "                    emb = Reshape(target_shape=(self.__K[col],))(emb)\n",
    "\n",
    "                    embeddings.append(emb)\n",
    "\n",
    "                if (len(self.__Lcat) > 1):\n",
    "                    emb_layer = concatenate(embeddings)\n",
    "                else:\n",
    "                    emb_layer = embeddings[0]\n",
    "\n",
    "                lay1 = Dense(n_layer1,\n",
    "                             kernel_initializer='uniform',\n",
    "                             activation='relu')(emb_layer)\n",
    "                lay1 = Dropout(dropout1)(lay1)\n",
    "\n",
    "                lay2 = Dense(n_layer2,\n",
    "                             kernel_initializer='uniform',\n",
    "                             activation='relu')(lay1)\n",
    "                lay2 = Dropout(dropout2)(lay2)\n",
    "\n",
    "                # Learning the weights\n",
    "\n",
    "                if ((y_train.dtype == object) | (y_train.dtype == 'int')):\n",
    "\n",
    "                    # Classification\n",
    "                    if (y_train.nunique() == 2):\n",
    "\n",
    "                        outputs = Dense(1,\n",
    "                                        kernel_initializer='normal',\n",
    "                                        activation='sigmoid')(lay2)\n",
    "\n",
    "                        model = Model(inputs=inputs, outputs=outputs)\n",
    "                        model.compile(loss='binary_crossentropy',\n",
    "                                      optimizer='adam')\n",
    "                        model.fit(\n",
    "                            [df_train[col].apply(lambda x: self.__Enc[col][x]).values\n",
    "                             for col in self.__Lcat],\n",
    "                            pd.get_dummies(y_train,\n",
    "                                           drop_first=True).astype(int).values,\n",
    "                            epochs=epochs,\n",
    "                            batch_size=batch_size,\n",
    "                            verbose=int(self.verbose)\n",
    "                        )\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        outputs = Dense(y_train.nunique(),\n",
    "                                        kernel_initializer='normal',\n",
    "                                        activation='softmax')(lay2)\n",
    "\n",
    "                        model = Model(inputs=inputs, outputs=outputs)\n",
    "                        model.compile(loss='binary_crossentropy',\n",
    "                                      optimizer='adam')\n",
    "                        model.fit(\n",
    "                            [df_train[col].apply(lambda x: self.__Enc[col][x]).values\n",
    "                             for col in self.__Lcat],\n",
    "                            pd.get_dummies(y_train,\n",
    "                                           drop_first=False).astype(int).values,\n",
    "                            epochs=epochs,\n",
    "                            batch_size=batch_size,\n",
    "                            verbose=int(self.verbose)\n",
    "                        )\n",
    "\n",
    "                else:\n",
    "\n",
    "                    # Regression\n",
    "                    outputs = Dense(1, kernel_initializer='normal')(lay2)\n",
    "                    model = Model(inputs=inputs, outputs=outputs)\n",
    "                    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "                    model.fit(\n",
    "                        [df_train[col].apply(lambda x: self.__Enc[col][x]).values\n",
    "                         for col in self.__Lcat],\n",
    "                        y_train.values,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        verbose=int(self.verbose)\n",
    "                    )\n",
    "\n",
    "                self.__weights = model.get_weights()\n",
    "\n",
    "                self.__fitOK = True\n",
    "\n",
    "            #################################################\n",
    "            #                Random Projection\n",
    "            #################################################\n",
    "\n",
    "            elif(self.strategy == 'random_projection'):\n",
    "\n",
    "                for col in self.__Lcat:\n",
    "\n",
    "                    exp_ = np.exp(-df_train[col].nunique() * 0.05)\n",
    "                    # TODO: Add reference to formula used here below?\n",
    "                    self.__K[col] = np.int(5 * (1 - exp_)) + 1\n",
    "\n",
    "                    d = dict()\n",
    "                    levels = list(df_train[col].unique())\n",
    "                    nan = False\n",
    "\n",
    "                    if np.NaN in levels:\n",
    "                        nan = True\n",
    "                        levels.remove(np.NaN)\n",
    "\n",
    "                    for k in range(self.__K[col]):\n",
    "\n",
    "                        if (k == 0):\n",
    "                            levels = sorted(levels)\n",
    "\n",
    "                        else:\n",
    "                            np.random.seed(k)\n",
    "                            np.random.shuffle(levels)\n",
    "\n",
    "                        for enc, level in enumerate([np.NaN] * nan + levels):\n",
    "                            if(k == 0):\n",
    "                                d[level] = [enc]\n",
    "                            else:\n",
    "                                d[level] = d[level] + [enc]\n",
    "\n",
    "                    self.__Enc[col] = d\n",
    "\n",
    "                self.__fitOK = True\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\"Categorical encoding strategy is not valid\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, df_train, y_train):\n",
    "        \"\"\"Fits Categorical Encoder and transforms the dataset.\n",
    "\n",
    "        Fit categorical encoder following strategy parameter and transform the\n",
    "        dataset df_train.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df_train : pandas.Dataframe of shape = (n_train, n_features)\n",
    "            The training dataset with numerical and categorical features.\n",
    "            NA values are allowed.\n",
    "        y_train : pandas.Series of shape = (n_train, ).\n",
    "            The target for classification or regression tasks.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pandas.Dataframe of shape = (n_train, n_features)\n",
    "            Training dataset with numerical and encoded categorical features.\n",
    "\n",
    "        \"\"\"\n",
    "        self.fit(df_train, y_train)\n",
    "\n",
    "        return self.transform(df_train)\n",
    "\n",
    "    def transform(self, df):\n",
    "        \"\"\"Transform categorical variable of df dataset.\n",
    "\n",
    "        Transform df DataFrame encoding categorical features with the strategy\n",
    "        parameter if self.__fitOK is set to True.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas.Dataframe of shape = (n_train, n_features)\n",
    "            The training dataset with numerical and categorical features.\n",
    "            NA values are allowed.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pandas.Dataframe of shape = (n_train, n_features)\n",
    "            The dataset with numerical and encoded categorical features.\n",
    "\n",
    "        \"\"\"\n",
    "        if self.__fitOK:\n",
    "\n",
    "            if len(self.__Lcat) == 0:\n",
    "                return df[self.__Lnum]\n",
    "\n",
    "            else:\n",
    "\n",
    "                #################################################\n",
    "                #                Label Encoding\n",
    "                #################################################\n",
    "\n",
    "                if (self.strategy == 'label_encoding'):\n",
    "\n",
    "                    for col in self.__Lcat:\n",
    "\n",
    "                        # Handling unknown levels\n",
    "                        unknown_levels = list(set(df[col].values)\n",
    "                                              - set(self.__Enc[col].keys()))\n",
    "\n",
    "                        if (len(unknown_levels) != 0):\n",
    "\n",
    "                            new_enc = len(self.__Enc[col])\n",
    "\n",
    "                            for unknown_level in unknown_levels:\n",
    "\n",
    "                                d = self.__Enc[col]\n",
    "                                # TODO: make sure no collisions introduced\n",
    "                                d[unknown_level] = new_enc\n",
    "                                self.__Enc[col] = d\n",
    "\n",
    "                    if (len(self.__Lnum) == 0):\n",
    "                        return pd.concat(\n",
    "                            [pd.DataFrame(\n",
    "                                df[col].apply(lambda x: self.__Enc[col][x]).values,\n",
    "                                columns=[col], index=df.index\n",
    "                                         ) for col in self.__Lcat],\n",
    "                            axis=1)[df.columns]\n",
    "                    else:\n",
    "                        return pd.concat(\n",
    "                            [df[self.__Lnum]]\n",
    "                            + [pd.DataFrame(\n",
    "                                df[col].apply(lambda x: self.__Enc[col][x]).values,\n",
    "                                columns=[col],\n",
    "                                index=df.index\n",
    "                                ) for col in self.__Lcat],\n",
    "                            axis=1)[df.columns]\n",
    "\n",
    "                #################################################\n",
    "                #                 Dummification\n",
    "                #################################################\n",
    "\n",
    "                elif (self.strategy == 'dummification'):\n",
    "\n",
    "                    sub_var = []\n",
    "                    missing_var = []\n",
    "\n",
    "                    for col in self.__Lcat:\n",
    "\n",
    "                        # Handling unknown and missing levels\n",
    "                        unique_levels = set(df[col].values)\n",
    "                        sub_levels = unique_levels & set(self.__Enc[col])\n",
    "                        missing_levels = [col + \"_\" + str(s)\n",
    "                                          for s in list(set(self.__Enc[col]) - sub_levels)]\n",
    "                        sub_levels = [col + \"_\" + str(s)\n",
    "                                      for s in list(sub_levels)]\n",
    "\n",
    "                        sub_var = sub_var + sub_levels\n",
    "                        missing_var = missing_var + missing_levels\n",
    "\n",
    "                    if (len(missing_var) != 0):\n",
    "\n",
    "                        return pd.SparseDataFrame(\n",
    "                            pd.concat(\n",
    "                                [pd.get_dummies(df,\n",
    "                                                sparse=True)[list(self.__Lnum)\n",
    "                                                             + sub_var]]\n",
    "                                + [pd.DataFrame(np.zeros((df.shape[0],\n",
    "                                                          len(missing_var))),\n",
    "                                                columns=missing_var,\n",
    "                                                index=df.index)],\n",
    "                                axis=1\n",
    "                            )[list(self.__Lnum)+sorted(missing_var+sub_var)])\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        return pd.get_dummies(df, sparse=True)[list(self.__Lnum) + sorted(sub_var)]\n",
    "\n",
    "            #################################################\n",
    "            #               Entity Embedding\n",
    "            #################################################\n",
    "\n",
    "                elif (self.strategy == 'entity_embedding'):\n",
    "\n",
    "                    def get_embeddings(x, col, i):\n",
    "                        if int(self.__Enc[col][x]) < \\\n",
    "                                np.shape(self.__weights[i])[0]:\n",
    "                            return self.__weights[i][int(self.__Enc[col][x]), :]\n",
    "                        return np.mean(self.__weights[i], axis=0)\n",
    "\n",
    "                    for col in self.__Lcat:\n",
    "\n",
    "                        # Handling unknown levels\n",
    "                        unknown_levels = list(set(df[col].values)\n",
    "                                              - set(self.__Enc[col].keys())\n",
    "                                              )\n",
    "\n",
    "                        if (len(unknown_levels) != 0):\n",
    "\n",
    "                            new_enc = len(self.__Enc[col])\n",
    "\n",
    "                            for unknown_level in unknown_levels:\n",
    "\n",
    "                                d = self.__Enc[col]\n",
    "                                d[unknown_level] = new_enc\n",
    "                                self.__Enc[col] = d\n",
    "\n",
    "                    if (len(self.__Lnum) == 0):\n",
    "                        return pd.concat(\n",
    "                            [pd.DataFrame(\n",
    "                                df[col].apply(lambda x: get_embeddings(x, col, i)).tolist(),\n",
    "                                columns=[col + \"_emb\" + str(k + 1)\n",
    "                                         for k in range(self.__K[col])],\n",
    "                                index=df.index\n",
    "                            )\n",
    "                             for i, col in enumerate(self.__Lcat)], axis=1)\n",
    "                    else:\n",
    "                        return pd.concat(\n",
    "                            [df[self.__Lnum]]\n",
    "                            + [pd.DataFrame(\n",
    "                                df[col].apply(lambda x: get_embeddings(x, col, i)).tolist(),\n",
    "                                columns=[col + \"_emb\" + str(k + 1)\n",
    "                                         for k in range(self.__K[col])],\n",
    "                                index=df.index\n",
    "                            )\n",
    "                             for i, col in enumerate(self.__Lcat)], axis=1)\n",
    "\n",
    "            #################################################\n",
    "            #               Random Projection\n",
    "            #################################################\n",
    "\n",
    "                else:\n",
    "\n",
    "                    for col in self.__Lcat:\n",
    "\n",
    "                        unknown_levels = list(set(df[col].values)\n",
    "                                              - set(self.__Enc[col].keys())\n",
    "                                              )\n",
    "\n",
    "                        if (len(unknown_levels) != 0):\n",
    "\n",
    "                            new_enc = len(self.__Enc[col])\n",
    "\n",
    "                            for unknown_level in unknown_levels:\n",
    "\n",
    "                                d = self.__Enc[col]\n",
    "                                d[unknown_level] = [new_enc\n",
    "                                                    for _ in range(self.__K[col])]\n",
    "                                self.__Enc[col] = d\n",
    "\n",
    "                    if (len(self.__Lnum) == 0):\n",
    "                        return pd.concat(\n",
    "                            [pd.DataFrame(\n",
    "                                df[col].apply(lambda x: self.__Enc[col][x]).tolist(),\n",
    "                                columns=[col + \"_proj\" + str(k + 1)\n",
    "                                         for k in range(self.__K[col])],\n",
    "                                index=df.index\n",
    "                            ) for col in self.__Lcat], axis=1)\n",
    "                    else:\n",
    "                        return pd.concat(\n",
    "                            [df[self.__Lnum]]\n",
    "                            + [pd.DataFrame(\n",
    "                                df[col].apply(lambda x: self.__Enc[col][x]).tolist(),\n",
    "                                columns=[col + \"_proj\" + str(k + 1)\n",
    "                                         for k in range(self.__K[col])],\n",
    "                                index=df.index) for col in self.__Lcat], axis=1)\n",
    "\n",
    "        else:\n",
    "\n",
    "            raise ValueError(\"Call fit or fit_transform function before\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'encoding'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-da26376de6e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#from drift import drift_threshold as DriftThreshold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mna_encoder\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNA_encoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategorical_encoder\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCategorical_encoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'encoding'"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "# Author: Axel ARONIO DE ROMBLAY <axelderomblay@gmail.com>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import os\n",
    "import time\n",
    "from sklearn.pipeline import Pipeline\n",
    "#from drift import drift_threshold as DriftThreshold\n",
    "\n",
    "from encoding.na_encoder import NA_encoder\n",
    "from encoding.categorical_encoder import Categorical_encoder\n",
    "import sys\n",
    "sys.path = \"C:\\\\Users\\\\user\\\\Desktop\\\\my-vishnu-proj'\\\\preprocessing\\\\drift\"\n",
    "\n",
    "class Drift_thresholder():\n",
    "\n",
    "    \"\"\"Automatically drops ids and drifting variables between train and test datasets.\n",
    "\n",
    "    Drops on train and test datasets. The list of drift coefficients is available and\n",
    "    saved as \"drifts.txt\". To get familiar with drift:\n",
    "    https://github.com/AxeldeRomblay/MLBox/blob/master/docs/webinars/features.pdf\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    threshold : float, defaut = 0.6\n",
    "        Drift threshold under which features are kept. Must be between 0. and 1.\n",
    "        The lower the more you keep non-drifting/stable variables: a feature with\n",
    "        a drift measure of 0. is very stable and a one with 1. is highly unstable.\n",
    "\n",
    "    inplace : bool, default = False\n",
    "        If True, train and test datasets are transformed. Returns self.\n",
    "        Otherwise, train and test datasets are not transformed. Returns a new dictionnary with\n",
    "        cleaned datasets.\n",
    "\n",
    "    verbose : bool, default = True\n",
    "        Verbose mode\n",
    "\n",
    "    to_path : str, default = \"save\"\n",
    "        Name of the folder where the list of drift coefficients is saved.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 threshold=0.6,\n",
    "                 inplace=False,\n",
    "                 verbose=True,\n",
    "                 to_path=\"save\"):\n",
    "\n",
    "        self.threshold = threshold\n",
    "        self.inplace = inplace\n",
    "        self.verbose = verbose\n",
    "        self.to_path = to_path\n",
    "        self.__Ddrifts = {}\n",
    "        self.__fitOK = False\n",
    "\n",
    "\n",
    "    def fit_transform(self, df):\n",
    "\n",
    "        \"\"\"Fits and transforms train and test datasets\n",
    "\n",
    "        Automatically drops ids and drifting variables between train and test datasets.\n",
    "        The list of drift coefficients is available and saved as \"drifts.txt\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : dict, defaut = None\n",
    "            Dictionnary containing :\n",
    "\n",
    "            - 'train' : pandas dataframe for train dataset\n",
    "            - 'test' : pandas dataframe for test dataset\n",
    "            - 'target' : pandas serie for the target on train set\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Dictionnary containing :\n",
    "\n",
    "            - 'train' : transformed pandas dataframe for train dataset\n",
    "            - 'test' : transformed pandas dataframe for test dataset\n",
    "            - 'target' : pandas serie for the target on train set\n",
    "        \"\"\"\n",
    "\n",
    "        ######################################################\n",
    "        #                   Deleting IDs\n",
    "        ######################################################\n",
    "\n",
    "        # Exception\n",
    "\n",
    "        if (df[\"test\"].shape[0] == 0):\n",
    "            if (self.verbose):\n",
    "                print(\"\")\n",
    "                print(\"You have no test dataset...\")\n",
    "\n",
    "            return df\n",
    "\n",
    "        else:\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            ds = DriftThreshold(self.threshold)\n",
    "            na = NA_encoder(numerical_strategy=0)\n",
    "            ca = Categorical_encoder()\n",
    "\n",
    "            pp = Pipeline([(\"na\", na), (\"ca\", ca)])\n",
    "            pp.fit(df['train'], None)\n",
    "\n",
    "            # Deleting IDs with drift threshold method\n",
    "\n",
    "            if (self.verbose):\n",
    "                print(\"\")\n",
    "                print(\"computing drifts ...\")\n",
    "\n",
    "            ds.fit(pp.transform(df['train']), pp.transform(df['test']))\n",
    "\n",
    "            if (self.verbose):\n",
    "                print(\"CPU time: %s seconds\" % (time.time() - start_time))\n",
    "                print(\"\")\n",
    "\n",
    "            self.__fitOK = True\n",
    "            self.__Ddrifts = ds.drifts()\n",
    "            drifts_top = sorted(ds.drifts().items(),\n",
    "                                key=lambda x: x[1],\n",
    "                                reverse=True)[:10]\n",
    "\n",
    "            if (self.verbose):\n",
    "                print(\"> Top 10 drifts\")\n",
    "                print(\"\")\n",
    "                for d in range(len(drifts_top)):\n",
    "                    print(drifts_top[d])\n",
    "\n",
    "            if (self.verbose):\n",
    "                print(\"\")\n",
    "                print(\"> Deleted \"\n",
    "                      \"variables : \" + str(ds.get_support(complement=True)))\n",
    "\n",
    "            ######################################################\n",
    "            #           Dumping Encoders into directory\n",
    "            ######################################################\n",
    "\n",
    "            if (self.to_path is not None):\n",
    "\n",
    "                try:\n",
    "                    os.mkdir(self.to_path)\n",
    "                except OSError:\n",
    "                    pass\n",
    "\n",
    "                file = open(self.to_path + '/drifts.txt', \"w\")\n",
    "                file.write(\"\\n\")\n",
    "                file.write(\n",
    "                    \"*******************************************\"\n",
    "                    \"  Drifts coefficients \"\n",
    "                    \"*******************************************\\n\")\n",
    "                file.write(\"\\n\")\n",
    "\n",
    "                for var, d in sorted(ds.drifts().items(),\n",
    "                                     key=lambda x: x[1],\n",
    "                                     reverse=True):\n",
    "                    file.write(str(var) + \" = \" + str(d) + '\\n')\n",
    "\n",
    "                file.close()\n",
    "\n",
    "                if (self.verbose):\n",
    "                    print(\"> Drift coefficients dumped into directory : \" + self.to_path)\n",
    "\n",
    "            # Returning datasets with no IDs\n",
    "\n",
    "            if (self.inplace):\n",
    "\n",
    "                df['train'] = ds.transform(df['train'])\n",
    "                df['test'] = ds.transform(df['test'])\n",
    "\n",
    "            else:\n",
    "\n",
    "                return {'train': ds.transform(df['train']),\n",
    "                        'test': ds.transform(df['test']),\n",
    "                        'target': df['target']}\n",
    "\n",
    "    def drifts(self):\n",
    "\n",
    "        \"\"\"Returns the univariate drifts for all variables.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Dictionnary containing the drifts for each feature\n",
    "        \"\"\"\n",
    "\n",
    "        if self.__fitOK:\n",
    "\n",
    "            return self.__Ddrifts\n",
    "        else:\n",
    "            raise ValueError('Call the fit_transform function before !')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
